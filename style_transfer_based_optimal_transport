import torch
import torch.nn as nn
from torch.optim import Adam
from torchvision import transforms, models
from torchvision.utils import save_image
from PIL import Image
from tqdm import tqdm

def wasserstein_loss(pred, target, eps=1e-7, constant=12.8):
    """`Implementation of paper `Enhancing Geometric Factors into
    Model Learning and Inference for Object Detection and Instance
    Segmentation <https://arxiv.org/abs/2005.03572>`_.
    Code is modified from https://github.com/Zzh-tju/CIoU.
    Args:
        pred (Tensor): Predicted bboxes of format (x_center, y_center, w, h),
            shape (n, 4).
        target (Tensor): Corresponding gt bboxes, shape (n, 4).
        eps (float): Eps to avoid log(0).
    Return:
        Tensor: Loss tensor.
    """

    center1 = pred[:, :2]
    center2 = target[:, :2]

    whs = center1[:, :2] - center2[:, :2]

    center_distance = whs[:, 0] * whs[:, 0] + whs[:, 1] * whs[:, 1] + eps #

    w1 = pred[:, 2]  + eps
    h1 = pred[:, 3]  + eps
    w2 = target[:, 2] + eps
    h2 = target[:, 3] + eps

    wh_distance = ((w1 - w2) ** 2 + (h1 - h2) ** 2) / 4

    wasserstein_2 = center_distance + wh_distance
    return torch.exp(-torch.sqrt(wasserstein_2) / constant).sum()


# Define the transformation to preprocess the images
transform = transforms.Compose([transforms.ToTensor()])

# Load the source and target images as PIL Images
source_image = Image.open(r'D:\code\ot\target.jpg')
target_image = Image.open(r'D:\code\ot\source.jpg')

# Resize the images to have the same dimensions
source_image = source_image.resize((224, 224))
target_image = target_image.resize((224, 224))

# Convert the images to the desired format using the defined transformation
source_image = transform(source_image).unsqueeze(0)  # Add batch dimension
target_image = transform(target_image).unsqueeze(0)  # Add batch dimension

# Ensure target image has 3 dimensions (height, width, channels)
target_image = target_image[:, :source_image.shape[2], :source_image.shape[3]]

# Define the VGG19 model
vgg19 = models.vgg19(pretrained=True).features.eval()

# Extract the features from the source and target images
source_features = vgg19(source_image)
target_features = vgg19(target_image)

# Define the Wasserstein distance loss
# loss_fn = wasserstein_loss()

# Define the optimizer
optimizer = Adam([source_image.requires_grad_()], lr=0.01)

# Define the number of iterations
num_iterations = 5000

# Perform the style transfer
for i in tqdm(range(num_iterations)):
    optimizer.zero_grad()
    source_features = vgg19(source_image)
    loss = wasserstein_loss(source_features, target_features)
    if i % 100 == 0:
        print("loss:",loss)
    loss.backward(retain_graph=True)
    optimizer.step()

# Generate the stylized image
stylized_image = source_image.detach()

# Save the stylized image
save_image(stylized_image.squeeze(0), "stylized_image.png")  # Remove batch dimension before saving
